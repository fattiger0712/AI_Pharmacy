import threading
import time

import pyaudio
import wave
import numpy as np
from funasr import AutoModel
from config import MODEL_SETTINGS, HOTWORDS_PATH
from model_manager import asr_model

'''
# å®šä¹‰å…¨å±€å˜é‡å’Œé”
_model_cache = None
_model_lock = threading.Lock()
_model_ready = threading.Event()

def _preload_model():
    global _model_cache
    with _model_lock:
        if _model_cache is None:
            print("â³ å¼€å§‹é¢„åŠ è½½è¯­éŸ³è¯†åˆ«æ¨¡å‹...")
            _model_cache = AutoModel(**MODEL_SETTINGS)
            _model_ready.set()  # æ ‡è®°æ¨¡å‹å°±ç»ª
            print("âœ… æ¨¡å‹é¢„åŠ è½½å®Œæˆ")

# å¯åŠ¨é¢„åŠ è½½çº¿ç¨‹
threading.Thread(target=_preload_model, daemon=True).start()


class SpeechRecognizer:
    def __init__(self):
        # ç­‰å¾…æ¨¡å‹åŠ è½½å®Œæˆ
        print("ğŸ•’ ç­‰å¾…æ¨¡å‹åˆå§‹åŒ–...")
        _model_ready.wait()  # é˜»å¡ç›´åˆ°æ¨¡å‹å°±ç»ª

        global _model_cache
        self.model = _model_cache
        print("ğŸ‰ è¯­éŸ³è¯†åˆ«å™¨å‡†å¤‡å°±ç»ª")
'''
class SpeechRecognizer:
    def __init__(self):
        self.model = asr_model  # ç›´æ¥ä½¿ç”¨é¢„åŠ è½½æ¨¡å‹
        print("ğŸ‰ è¯­éŸ³è¯†åˆ«å™¨å‡†å¤‡å°±ç»ª")
        # ç¡¬ä»¶é€‚é…å‚æ•°
        self.audio_format = pyaudio.paInt16
        self.channels = 1
        self.sample_rate = 16000
        self.chunk_size = 1024
        self.silence_threshold = 600  # ç¬”è®°æœ¬éº¦å…‹é£å»ºè®®600-800
        self.min_recording_sec = 0.8
        self.max_silent_sec = 3.5

        # åˆå§‹åŒ–FunASRæ¨¡å‹
        self.model = AutoModel(
            model=MODEL_SETTINGS["model"],
            disable_update=True,
            vad_model=MODEL_SETTINGS["vad_model"],
            punc_model=MODEL_SETTINGS["punc_model"],
            device=MODEL_SETTINGS["device"],
            quantize=True  # å¯ç”¨8ä½é‡åŒ–åŠ é€Ÿ
        )

    def _record_audio(self, timeout=5):
        pa = pyaudio.PyAudio()

        # æ˜¾å¼æŒ‡å®šç¬”è®°æœ¬å†…ç½®éº¦å…‹é£
        input_device_index = None
        for i in range(pa.get_device_count()):
            dev_info = pa.get_device_info_by_index(i)
            if "éº¦å…‹é£" in dev_info["name"] and dev_info["maxInputChannels"] > 0:
                input_device_index = i
                break

        stream = pa.open(
            format=self.audio_format,
            channels=self.channels,
            rate=self.sample_rate,
            input=True,
            input_device_index=input_device_index,
            frames_per_buffer=self.chunk_size
        )

        frames = []
        silent_frames = 0
        max_silent_sec = 3  # å…è®¸çš„æœ€å¤§é™éŸ³æ—¶é—´(1.5)

        print("\nè¯·æè¿°ç—‡çŠ¶ï¼ˆ5ç§’å†…ï¼‰ï¼š")
        for _ in range(int(self.sample_rate / self.chunk_size * timeout)):
            data = stream.read(self.chunk_size)
            audio_data = np.frombuffer(data, dtype=np.int16)

            # å®æ—¶é™éŸ³æ£€æµ‹
            if np.abs(audio_data).mean() < self.silence_threshold:
                silent_frames += 1
                if silent_frames > (max_silent_sec * self.sample_rate / self.chunk_size):
                    break
            else:
                silent_frames = 0
                frames.append(data)

        # ä¿å­˜ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
        wav_path = "temp_recording.wav"
        with wave.open(wav_path, 'wb') as wf:
            wf.setnchannels(self.channels)
            wf.setsampwidth(pa.get_sample_size(self.audio_format))
            wf.setframerate(self.sample_rate)
            wf.writeframes(b''.join(frames))

        stream.stop_stream()
        stream.close()
        pa.terminate()
        return wav_path

    def recognize(self):
        """æ‰§è¡Œè¯­éŸ³è¯†åˆ«"""
        try:
            audio_path = self._record_audio()
            result = self.model.generate(
                input=audio_path,
                hotwords=HOTWORDS_PATH  # åŠ è½½çƒ­è¯æ–‡ä»¶
            )
            return result[0]['text'].strip()
        except Exception as e:
            print(f"è¯­éŸ³è¯†åˆ«é”™è¯¯: {str(e)}")
            return ""